{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428c24d-7bb4-4a43-97bc-45a44cb38572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import model_selection\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.symbols import ORTH\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import luima_sbd.sbd_utils as sbd\n",
    "import os\n",
    "import fasttext\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#show all outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "def fig_prop():\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.ticklabel_format(style='plain', axis='x')\n",
    "    \n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 13})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2102f4-2341-4d42-bcd9-a6f4a0b7666f",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd8b90-666d-4f4e-b0ee-4c3593e5ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_model = fasttext.load_model(\"result_model.bin\")\n",
    "best_model = joblib.load('RSVM_best_model.joblib')\n",
    "\n",
    "train_tokens_mean = 20.592041800643088\n",
    "train_tokens_std = 15.221984070624842\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "special_cases = ['Vet. App.','Fed. Cir.']\n",
    "nlp.tokenizer.add_special_case('Vet. App.', [{ORTH: 'Vet. App.'}])\n",
    "nlp.tokenizer.add_special_case('Fed. Cir.', [{ORTH: 'Fed. Cir.'}])\n",
    "\n",
    "def spacy_tokenize(txt):\n",
    "    doc = nlp(txt)\n",
    "    tokens = list(doc)\n",
    "    clean_tokens = []\n",
    "    for i in range(len(tokens)):\n",
    "        t = tokens[i]\n",
    "        #print(t.pos_, t.text)\n",
    "        #print(i, len(tokens))\n",
    "        if(i != len(tokens) - 1):\n",
    "            t_next = tokens[i+1]\n",
    "        else: t_next = None\n",
    "        if(t_next!=None and t_next.pos_=='PART' and re.search(r'\\'', t_next.text)):\n",
    "            t_combined = t.text + t_next.text\n",
    "            t_combined = re.sub(r'\\W','',t_combined).lower()\n",
    "            clean_tokens.append(t_combined)\n",
    "            i+=1           \n",
    "        elif t.pos_ == 'PUNCT':\n",
    "            pass\n",
    "        elif t.text in special_cases:\n",
    "            clean_tokens.append(t.lemma_.lower())\n",
    "        elif (t.text[0].isalpha() == False and t.is_digit==False and t.is_upper == False):\n",
    "            pass            \n",
    "        elif t.pos_ == 'NUM':\n",
    "            clean_tokens.append(f'<NUM{len(t)}>')\n",
    "        else:\n",
    "            lemma = t.lemma_\n",
    "            lemma = re.sub(r'\\W','',lemma)\n",
    "            lemma =lemma.lower()\n",
    "            clean_tokens.append(lemma)\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def spans_add_spacy_tokens(spans):\n",
    "    for s in spans:\n",
    "        s['tokens_spacy'] = spacy_tokenize(s['txt'])\n",
    "        s['tokens_count'] = len(s['tokens_spacy'])\n",
    "        \n",
    "def word_vector_spans(spans):\n",
    "    for s in spans:\n",
    "        total_vec = np.zeros(100,)\n",
    "        total_tokens = s['tokens_count']\n",
    "        if(total_tokens != 0):\n",
    "            for t in s['tokens_spacy']:\n",
    "                word_vec = embedding_model.get_word_vector(t)\n",
    "                total_vec = np.add(total_vec, word_vec)\n",
    "            average_vec = total_vec / total_tokens\n",
    "            s['average_vec'] = average_vec\n",
    "        else:\n",
    "            s['average_vec'] = np.zeros(100,)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d5bc2-2801-45c4-988c-5877bf7ed75b",
   "metadata": {},
   "source": [
    "# ANALYZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca25769-5def-4e1a-a313-38c4591cda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = joblib.load('spacy_vectorizer.joblib')\n",
    "\n",
    "def make_spans_data(sentences, offsets, counts):\n",
    "    spans_data = []\n",
    "    for i in range(len(sentences)):\n",
    "        span_dict = {\n",
    "            'txt': sentences[i],\n",
    "            'start_normalized': offsets[i][0]/counts\n",
    "        }\n",
    "        spans_data.append(span_dict)\n",
    "        \n",
    "    return spans_data\n",
    "\n",
    "def make_feature_vectors_and_labels(spans):\n",
    "    # function takes long to execute\n",
    "    # note: we un-sparse the matrix here to be able to manipulate it\n",
    "    #tfidf = vectorizer.transform([s['txt'] for s in spans]).toarray()\n",
    "    starts_normalized = np.array([s['start_normalized'] for s in spans])\n",
    "    \n",
    "    num_tokens_normalized = np.array([(s['tokens_count']-train_tokens_mean)/train_tokens_std for s in spans])\n",
    "    \n",
    "    avg_vec = np.array([s['average_vec'] for s in spans])\n",
    "    X = np.concatenate((avg_vec, np.expand_dims(starts_normalized, axis=1), np.expand_dims(num_tokens_normalized, axis=1)), axis=1)\n",
    "    return X\n",
    "\n",
    "\n",
    "def analyze(text):\n",
    "\n",
    "    results = []\n",
    "    predictions = []\n",
    "    length = len(text)\n",
    "    \n",
    "    sentences = sbd.text2sentences(text, offsets=False)\n",
    "    offsets = sbd.text2sentences(text, offsets=True)\n",
    "    \n",
    "    spans_data = make_spans_data(sentences, offsets, length)\n",
    "    spans_add_spacy_tokens(spans_data)\n",
    "    word_vector_spans(spans_data)\n",
    "    test_data = make_feature_vectors_and_labels(spans_data)\n",
    "    \n",
    "    predictions = best_model.predict(test_data)\n",
    "    for i in range(len(sentences)):\n",
    "        arr = [sentences[i], predictions[i]]\n",
    "        results.append(arr)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901779a1-e4dd-4c08-888d-59b306caa9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
